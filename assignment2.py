import nltk
import sys
import random
from nltk.corpus import PlaintextCorpusReader
from nltk.text import Text
from nltk import word_tokenize
from nltk.util import ngrams
from nltk.probability import FreqDist,ConditionalFreqDist,ConditionalProbDist
from collections import defaultdict
from nltk.corpus import gutenberg

def read_files(ngramModel, numFiles):
    '''
    This function fetches text from all the files provided in the arguments and merges the files.
    This function returns tokens as the output
    '''
    MergedText = []
    for file in range(3,numFiles):
        #Read all the files
	
        word =gutenberg.words(sys.argv[file])
        MergedText.extend(word)   #add
        
    return MergedText

def boundaries(nngrams):  
    ''' delete the cross boudary nngrams'''
    boundary=['.','?',';',',','!']
    new_nngrams=[]
    for i in nngrams:
        for j in boundary:
            if j in i and i.index(j)!=0 and i.index(j)!=len(i)-1:
                break
            if j==boundary[-1]:
                new_nngrams.append(i)
    return new_nngrams

def delete_short(words,ngramModel):
    '''delete the words from short sentence which words amount is less than n'''
    boundary=['.','?',';',',','!']
    new_words=[]
    sentence=[]
    for i in words:       
        sentence.append(i.lower())
        if i in boundary: 
            if len(sentence)>ngramModel: new_words.extend(sentence)
            sentence=[]
    return new_words

def generateModel(MergedText, MergedWord, ngramModel, numSentences):
    '''
    This function generates nGram Model.
    '''
    vocabulary = set(MergedText)
	
    MergedText=delete_short(MergedText,ngramModel) # delete the words from short sentence which words amount is less than n

    nngrams = boundaries(list(ngrams(MergedText,ngramModel)))  #generate the nngrams withour cross bouodary
	
    cfd = ConditionalFreqDist()
    ngramSet = set()
    vocabularyOfWords = set()
    fdist = FreqDist()
	
    ProbDictionary = defaultdict(list)
	#Generate conditional frequency distribution 
    
    for ngram in nngrams:
        ngramSet.add(ngram)
        initial_text = tuple(ngram[:-1])
        last_word = ngram[-1]
        cfd[initial_text][last_word] += 1
		
        #Smoothing and generating probabilities using Laplace Algorithm
        laplace_prob = [1.0 * (1+cfd[initial_text][last_word]) / (len(vocabulary)+cfd[initial_text].N())]
        ProbDictionary[initial_text].append(last_word)
        vocabularyOfWords.add(last_word)
   
    generateSentences(nngrams,cfd,ngramModel,numSentences)
	    
	
def generateSentences(nngrams,cfd,ngramModel,numSentences):
    
    for sentence in range(int(numSentences)):
	
        #select a random ngram out of all ngrams generated
        random_ngram = random.choice(nngrams)

        #get rid of the last word from the ngram, "seed" is a list of string with the first (n-1) words
        seed = tuple(random_ngram[:-1])
        #print("seed: ",seed)

        #Predict the next word based on the seed
        #predictedWord = cfd[seed].max()
        for key,value in ProbDictionary.items():
            if(key == seed):
                predictedWord = random.choice(value)
		
        #New text generated by joining seed and the predicted word
        newText = ' '.join(seed) + " " + predictedWord

        #Predict next 15 words for new sentence creation
        for words in range(15):
            #Add the word predicted to the actual seed
            seed+= tuple([predictedWord])

            #Convert the seed to a list
            seedList = list(seed)
            
            #Remove the first element from the seedList to create a new seed...slides to the next seed
            seedList.pop(0)
            
            #Convert the updated seed back to the seed tuple
            seed = tuple(seedList)
			
            #Predict the next word based on the new seed
            predictedWord = cfd[seed].max()
		
            #New Text is created by joining previous newText and the next predicted word
            newText = newText + " " + predictedWord
        print("New Sentence: ", newText)

def main():
    '''
    This is the main function. 
	'''
    
    ngramModel= int(sys.argv[1])
    numSentences = sys.argv[2] 
    numFiles = len(sys.argv)	
    MergedText = read_files(ngramModel,numFiles)
    print "This program generates random sentences based on an Ngram model."
    print "Command line settings: ngram.py",ngramModel,numSentences
    generateModel(MergedText,ngramModel,numSentences)
  
    
if __name__ == '__main__':
    main()
